{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792dc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "ALPHA_VANTAGE_API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')\n",
    "\n",
    "class EnhancedDeltaDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with optional data augmentation\"\"\"\n",
    "    def __init__(self, features, targets, augment=False):\n",
    "        self.features = torch.from_numpy(features.astype(np.float32))\n",
    "        self.targets = torch.from_numpy(targets.astype(np.float32))\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        targets = self.targets[idx]\n",
    "        \n",
    "        if self.augment and torch.rand(1) > 0.5:\n",
    "            # Add small gaussian noise for regularization\n",
    "            noise = torch.normal(0, 0.01, features.shape)\n",
    "            features = features + noise\n",
    "            \n",
    "        return features, targets\n",
    "\n",
    "class TransformerDeltaPredictor(nn.Module):\n",
    "    \"\"\"Transformer-based delta predictor with attention mechanisms\"\"\"\n",
    "    def __init__(self, input_size=7, d_model=128, nhead=8, num_layers=4, dropout=0.2):\n",
    "        super(TransformerDeltaPredictor, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(100, d_model))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Feature-wise attention\n",
    "        self.feature_attention = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, d_model // 4),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(d_model // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Temperature parameter for calibration\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project input features to transformer dimension\n",
    "        x = self.input_projection(x)  # (batch, d_model)\n",
    "        \n",
    "        # Add positional encoding and expand for sequence modeling\n",
    "        x = x.unsqueeze(1) + self.pos_encoding[0:1, :]  # (batch, 1, d_model)\n",
    "        \n",
    "        # Self-attention through transformer\n",
    "        transformer_out = self.transformer(x)  # (batch, 1, d_model)\n",
    "        \n",
    "        # Feature attention mechanism\n",
    "        attended_out, attention_weights = self.feature_attention(\n",
    "            transformer_out, transformer_out, transformer_out\n",
    "        )\n",
    "        \n",
    "        # Combine transformer and attention outputs\n",
    "        combined = transformer_out + attended_out  # Residual connection\n",
    "        \n",
    "        # Pool and output\n",
    "        pooled = combined.squeeze(1)  # (batch, d_model)\n",
    "        \n",
    "        # Temperature-scaled output\n",
    "        output = self.output_layers(pooled)\n",
    "        return output / self.temperature\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    \"\"\"1D CNN for extracting local patterns in option features\"\"\"\n",
    "    def __init__(self, input_size=7, hidden_channels=64, output_size=32):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # 1D Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels * 2),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size)\n",
    "        )\n",
    "        \n",
    "        self.output_projection = nn.Linear(hidden_channels * 2 * output_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape for 1D conv: (batch, 1, features)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Extract features through conv layers\n",
    "        conv_out = self.conv_layers(x)  # (batch, hidden_channels*2, output_size)\n",
    "        \n",
    "        # Flatten and project\n",
    "        flattened = conv_out.view(conv_out.size(0), -1)\n",
    "        return self.output_projection(flattened)\n",
    "\n",
    "class LSTMDeltaPredictor(nn.Module):\n",
    "    \"\"\"LSTM-based model for sequential option data modeling\"\"\"\n",
    "    def __init__(self, input_size=7, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super(LSTMDeltaPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape for LSTM: (batch, seq_len=1, features)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden_size*2)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        \n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # (batch, hidden_size*2)\n",
    "        \n",
    "        return self.output(context)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.linear1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.linear2(out))\n",
    "        \n",
    "        # Residual connection\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "class EnsembleDeltaPredictor(nn.Module):\n",
    "    \"\"\"Ensemble model combining multiple architectures\"\"\"\n",
    "    def __init__(self, input_size=7, hidden_size=128):\n",
    "        super(EnsembleDeltaPredictor, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Multiple model architectures\n",
    "        self.transformer_model = TransformerDeltaPredictor(input_size, d_model=hidden_size)\n",
    "        self.lstm_model = LSTMDeltaPredictor(input_size, hidden_size=hidden_size)\n",
    "        self.cnn_extractor = CNNFeatureExtractor(input_size, output_size=hidden_size//2)\n",
    "        \n",
    "        # Traditional feedforward network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            ResidualBlock(hidden_size, 0.1),\n",
    "            ResidualBlock(hidden_size, 0.1),\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size//2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Feature fusion network\n",
    "        fusion_input_size = hidden_size//2 + 1  # CNN features + feedforward output\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_input_size, hidden_size//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size//2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Learnable ensemble weights\n",
    "        self.ensemble_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "        \n",
    "        # Confidence predictors\n",
    "        self.confidence_nets = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(input_size, 32), nn.ReLU(), nn.Linear(32, 1), nn.Sigmoid())\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get predictions from each model\n",
    "        transformer_pred = self.transformer_model(x)\n",
    "        lstm_pred = self.lstm_model(x)\n",
    "        \n",
    "        # CNN feature extraction and feedforward prediction\n",
    "        cnn_features = self.cnn_extractor(x)\n",
    "        ff_pred = self.feedforward(x)\n",
    "        \n",
    "        # Feature fusion\n",
    "        fused_features = torch.cat([cnn_features, ff_pred], dim=1)\n",
    "        fusion_pred = self.feature_fusion(fused_features)\n",
    "        \n",
    "        # Confidence-weighted ensemble\n",
    "        confidences = torch.stack([\n",
    "            self.confidence_nets[0](x),\n",
    "            self.confidence_nets[1](x),\n",
    "            self.confidence_nets[2](x)\n",
    "        ], dim=2).squeeze(1)  # (batch, 3)\n",
    "        \n",
    "        # Normalize ensemble weights\n",
    "        normalized_weights = F.softmax(self.ensemble_weights, dim=0)\n",
    "        \n",
    "        # Weighted predictions\n",
    "        predictions = torch.stack([transformer_pred, lstm_pred, fusion_pred], dim=2).squeeze(1)\n",
    "        \n",
    "        # Confidence-adjusted weights\n",
    "        adjusted_weights = normalized_weights.unsqueeze(0) * confidences\n",
    "        adjusted_weights = adjusted_weights / adjusted_weights.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Final ensemble prediction\n",
    "        ensemble_pred = torch.sum(predictions * adjusted_weights, dim=1, keepdim=True)\n",
    "        \n",
    "        return ensemble_pred\n",
    "\n",
    "class EnhancedLiveDeltaPredictor:\n",
    "    def __init__(self, api_key=None):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://www.alphavantage.co/query\"\n",
    "        self.scaler = RobustScaler()\n",
    "        self.model = None\n",
    "        self.uncertainty_model = None\n",
    "        self.model_trained = False\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.current_stock_price = None\n",
    "        self.training_history = {'loss': [], 'val_loss': [], 'lr': []}\n",
    "        \n",
    "        print(f\"PyTorch Device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "    def get_sample_data(self):\n",
    "        \"\"\"Get real options data from Alpha Vantage API\"\"\"\n",
    "        print(\"Retrieving real options data from Alpha Vantage...\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Alpha Vantage API key is required. Set ALPHA_VANTAGE_API_KEY environment variable.\")\n",
    "        \n",
    "        try:\n",
    "            # Try to get real options data\n",
    "            params = {\n",
    "                'function': 'HISTORICAL_OPTIONS',\n",
    "                'symbol': 'AAPL',\n",
    "                'apikey': self.api_key\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.base_url, params=params, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'data' in data and len(data['data']) > 0:\n",
    "                return self.parse_alpha_vantage_data(data['data'])\n",
    "            else:\n",
    "                # If no options data, try stock data instead\n",
    "                print(\"No options data available, attempting to use stock data for demo...\")\n",
    "                return self.get_stock_based_data()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"API request failed: {str(e)}\")\n",
    "            raise RuntimeError(f\"Could not retrieve real market data: {str(e)}\")\n",
    "    \n",
    "    def parse_alpha_vantage_data(self, raw_data):\n",
    "        \"\"\"Parse Alpha Vantage options data\"\"\"\n",
    "        options_list = []\n",
    "        \n",
    "        for item in raw_data:\n",
    "            try:\n",
    "                options_list.append({\n",
    "                    'symbol': item.get('symbol'),\n",
    "                    'type': item.get('type', 'call'),\n",
    "                    'strike': float(item.get('strike', 0)),\n",
    "                    'expiration': item.get('expiration'),\n",
    "                    'bid': float(item.get('bid', 0)),\n",
    "                    'ask': float(item.get('ask', 0)),\n",
    "                    'volume': int(item.get('volume', 0)),\n",
    "                    'open_interest': int(item.get('open_interest', 0)),\n",
    "                    'implied_volatility': float(item.get('implied_volatility', 0)),\n",
    "                    'delta': float(item.get('delta', 0))\n",
    "                })\n",
    "            except (ValueError, TypeError) as e:\n",
    "                continue  # Skip malformed records\n",
    "        \n",
    "        if len(options_list) < 50:\n",
    "            raise ValueError(f\"Insufficient options data: only {len(options_list)} records\")\n",
    "        \n",
    "        return pd.DataFrame(options_list)\n",
    "    \n",
    "    def get_stock_based_data(self):\n",
    "        \"\"\"Fallback: Get stock data and create basic options framework for demo\"\"\"\n",
    "        params = {\n",
    "            'function': 'TIME_SERIES_DAILY',\n",
    "            'symbol': 'AAPL',\n",
    "            'apikey': self.api_key,\n",
    "            'outputsize': 'compact'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(self.base_url, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'Time Series (Daily)' not in data:\n",
    "            raise ValueError(\"No stock data available\")\n",
    "        \n",
    "        # Get recent stock price\n",
    "        time_series = data['Time Series (Daily)']\n",
    "        latest_date = max(time_series.keys())\n",
    "        current_price = float(time_series[latest_date]['4. close'])\n",
    "        \n",
    "        print(f\"Note: Using stock price ${current_price:.2f} for demo framework\")\n",
    "        print(\"Warning: This creates a framework only - real options data needed for production\")\n",
    "        \n",
    "        # Create minimal framework - NOT for actual trading\n",
    "        demo_options = []\n",
    "        strikes = np.arange(current_price * 0.85, current_price * 1.15, 5)\n",
    "        expirations = [7, 14, 30, 45, 60]\n",
    "        \n",
    "        for strike in strikes:\n",
    "            for days in expirations:\n",
    "                # Basic Black-Scholes approximation for demo structure only\n",
    "                moneyness = strike / current_price\n",
    "                time_factor = np.sqrt(days / 365)\n",
    "                \n",
    "                # Rough delta approximation (NOT for real trading)\n",
    "                if moneyness <= 1:\n",
    "                    approx_delta = 0.5 + 0.3 * (1 - moneyness) + 0.1 * time_factor\n",
    "                else:\n",
    "                    approx_delta = 0.5 - 0.3 * (moneyness - 1) + 0.1 * time_factor\n",
    "                \n",
    "                approx_delta = max(0.01, min(0.99, approx_delta))\n",
    "                \n",
    "                demo_options.append({\n",
    "                    'symbol': 'AAPL',\n",
    "                    'type': 'call',\n",
    "                    'strike': strike,\n",
    "                    'expiration': (datetime.now() + timedelta(days=days)).strftime('%Y-%m-%d'),\n",
    "                    'bid': max(0.01, current_price - strike + np.random.uniform(-2, 2)),\n",
    "                    'ask': max(0.05, current_price - strike + np.random.uniform(-1, 3)),\n",
    "                    'volume': np.random.randint(1, 1000),\n",
    "                    'open_interest': np.random.randint(10, 5000),\n",
    "                    'implied_volatility': np.random.uniform(0.15, 0.45),\n",
    "                    'delta': approx_delta\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(demo_options)\n",
    "    \n",
    "    def prepare_enhanced_training_data(self, options_df):\n",
    "        \"\"\"Enhanced feature engineering with real market data only\"\"\"\n",
    "        print(\"Preparing enhanced training data...\")\n",
    "        \n",
    "        # Filter for valid call options only\n",
    "        calls = options_df[\n",
    "            (options_df['type'] == 'call') & \n",
    "            (options_df['delta'] > 0.01) & \n",
    "            (options_df['delta'] < 0.99) &\n",
    "            (options_df['implied_volatility'] > 0.05) &\n",
    "            (options_df['implied_volatility'] < 1.0) &\n",
    "            (options_df['volume'] > 0) &\n",
    "            (options_df['strike'] > 0)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(calls) < 100:\n",
    "            raise ValueError(f\"Insufficient clean data: only {len(calls)} valid call options\")\n",
    "        \n",
    "        # Parse expiration dates\n",
    "        try:\n",
    "            calls['expiration_date'] = pd.to_datetime(calls['expiration'])\n",
    "            today = datetime.now()\n",
    "            calls['days_to_expiry'] = (calls['expiration_date'] - today).dt.days\n",
    "            \n",
    "            # Remove expired or invalid expiry dates\n",
    "            calls = calls[calls['days_to_expiry'] > 0]\n",
    "            calls = calls[calls['days_to_expiry'] <= 365]\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to process expiration dates: {e}\")\n",
    "        \n",
    "        if len(calls) < 100:\n",
    "            raise ValueError(\"Insufficient data after date filtering\")\n",
    "        \n",
    "        # Estimate current stock price\n",
    "        atm_options = calls[calls['delta'].between(0.45, 0.55)]\n",
    "        if len(atm_options) > 0:\n",
    "            self.current_stock_price = atm_options['strike'].median()\n",
    "        else:\n",
    "            self.current_stock_price = (calls['strike'] * calls['delta']).sum() / calls['delta'].sum()\n",
    "        \n",
    "        print(f\"Estimated stock price: ${self.current_stock_price:.2f}\")\n",
    "        \n",
    "        # Enhanced feature engineering\n",
    "        calls['moneyness'] = calls['strike'] / self.current_stock_price\n",
    "        calls['log_moneyness'] = np.log(calls['moneyness'])\n",
    "        calls['sqrt_time'] = np.sqrt(calls['days_to_expiry'] / 365)\n",
    "        \n",
    "        # Market microstructure features\n",
    "        calls['bid_ask_spread'] = calls['ask'] - calls['bid']\n",
    "        calls['mid_price'] = (calls['bid'] + calls['ask']) / 2\n",
    "        calls['volume_oi_ratio'] = calls['volume'] / np.maximum(calls['open_interest'], 1)\n",
    "        calls['log_volume'] = np.log1p(calls['volume'])\n",
    "        calls['log_open_interest'] = np.log1p(calls['open_interest'])\n",
    "        \n",
    "        # IV ranking\n",
    "        calls['iv_rank'] = calls.groupby('expiration')['implied_volatility'].rank(pct=True)\n",
    "        \n",
    "        # Time-scaled features\n",
    "        calls['moneyness_time'] = calls['moneyness'] * calls['sqrt_time'] \n",
    "        calls['iv_time'] = calls['implied_volatility'] * calls['sqrt_time']\n",
    "        calls['volume_time'] = calls['log_volume'] * calls['sqrt_time']\n",
    "        \n",
    "        # Define feature columns\n",
    "        feature_columns = [\n",
    "            'log_moneyness',\n",
    "            'sqrt_time', \n",
    "            'implied_volatility',\n",
    "            'log_volume',\n",
    "            'volume_oi_ratio',\n",
    "            'iv_rank',\n",
    "            'moneyness_time'\n",
    "        ]\n",
    "        \n",
    "        # Clean data\n",
    "        calls = calls.dropna(subset=feature_columns + ['delta'])\n",
    "        calls = calls[np.isfinite(calls[feature_columns + ['delta']]).all(axis=1)]\n",
    "        \n",
    "        # Remove outliers using IQR method\n",
    "        for col in feature_columns:\n",
    "            Q1 = calls[col].quantile(0.25)\n",
    "            Q3 = calls[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            calls = calls[(calls[col] >= lower_bound) & (calls[col] <= upper_bound)]\n",
    "        \n",
    "        if len(calls) < 100:\n",
    "            raise ValueError(\"Insufficient clean data after outlier removal\")\n",
    "        \n",
    "        features = calls[feature_columns].values\n",
    "        targets = calls['delta'].values.reshape(-1, 1)\n",
    "        \n",
    "        print(f\"Training data prepared: {len(features)} samples, {features.shape[1]} features\")\n",
    "        print(f\"Delta range: {targets.min():.4f} - {targets.max():.4f}\")\n",
    "        print(f\"IV range: {calls['implied_volatility'].min():.2%} - {calls['implied_volatility'].max():.2%}\")\n",
    "        \n",
    "        return features, targets\n",
    "    \n",
    "    def train_advanced_model(self, features, targets, epochs=100, batch_size=64, patience=15):\n",
    "        \"\"\"Advanced training with ensemble model\"\"\"\n",
    "        print(f\"Training ensemble model ({epochs} epochs, batch_size={batch_size})...\")\n",
    "        \n",
    "        # Data scaling\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        features_tensor = torch.from_numpy(features_scaled.astype(np.float32))\n",
    "        targets_tensor = torch.from_numpy(targets.astype(np.float32))\n",
    "        \n",
    "        # Split data\n",
    "        indices = torch.randperm(len(features_tensor))\n",
    "        split_idx = int(0.8 * len(features_tensor))\n",
    "        \n",
    "        train_indices = indices[:split_idx]\n",
    "        val_indices = indices[split_idx:]\n",
    "        \n",
    "        X_train = features_tensor[train_indices]\n",
    "        y_train = targets_tensor[train_indices]\n",
    "        X_val = features_tensor[val_indices]\n",
    "        y_val = targets_tensor[val_indices]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = EnhancedDeltaDataset(X_train.numpy(), y_train.numpy(), augment=True)\n",
    "        val_dataset = EnhancedDeltaDataset(X_val.numpy(), y_val.numpy(), augment=False)\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, \n",
    "            num_workers=0, pin_memory=True if torch.cuda.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=0, pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Initialize models\n",
    "        input_size = features.shape[1]\n",
    "        self.model = EnsembleDeltaPredictor(input_size=input_size).to(self.device)\n",
    "        \n",
    "        # Uncertainty predictor\n",
    "        self.uncertainty_model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Softplus()\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss functions\n",
    "        mse_loss = nn.MSELoss()\n",
    "        huber_loss = nn.HuberLoss(delta=0.1)\n",
    "        \n",
    "        # Optimizers\n",
    "        main_optimizer = optim.AdamW(\n",
    "            self.model.parameters(), lr=0.001, weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        uncertainty_optimizer = optim.RMSprop(\n",
    "            self.uncertainty_model.parameters(), lr=0.002, alpha=0.99\n",
    "        )\n",
    "        \n",
    "        # Schedulers\n",
    "        main_scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            main_optimizer, max_lr=0.01, \n",
    "            steps_per_epoch=len(train_loader), epochs=epochs,\n",
    "            pct_start=0.3, anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        uncertainty_scheduler = CosineAnnealingLR(\n",
    "            uncertainty_optimizer, T_max=epochs//4, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Mixed precision scaler\n",
    "        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Try torch.compile if available\n",
    "        try:\n",
    "            if hasattr(torch, 'compile') and torch.cuda.is_available():\n",
    "                self.model = torch.compile(self.model, mode='reduce-overhead')\n",
    "                print(\"Model compiled with torch.compile\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            self.uncertainty_model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (batch_features, batch_targets) in enumerate(train_loader):\n",
    "                batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                \n",
    "                main_optimizer.zero_grad()\n",
    "                uncertainty_optimizer.zero_grad()\n",
    "                \n",
    "                # Mixed precision forward pass\n",
    "                if scaler is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        delta_pred = self.model(batch_features)\n",
    "                        uncertainty_pred = self.uncertainty_model(batch_features)\n",
    "                        \n",
    "                        prediction_loss = 0.6 * mse_loss(delta_pred, batch_targets) + \\\n",
    "                                        0.4 * huber_loss(delta_pred, batch_targets)\n",
    "                        uncertainty_loss = uncertainty_pred.mean() * 0.01\n",
    "                        \n",
    "                        total_loss = prediction_loss + uncertainty_loss\n",
    "                    \n",
    "                    scaler.scale(total_loss).backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    scaler.unscale_(main_optimizer)\n",
    "                    scaler.unscale_(uncertainty_optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.uncertainty_model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    scaler.step(main_optimizer)\n",
    "                    scaler.step(uncertainty_optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # Standard precision training\n",
    "                    delta_pred = self.model(batch_features)\n",
    "                    uncertainty_pred = self.uncertainty_model(batch_features)\n",
    "                    \n",
    "                    prediction_loss = 0.6 * mse_loss(delta_pred, batch_targets) + \\\n",
    "                                    0.4 * huber_loss(delta_pred, batch_targets)\n",
    "                    uncertainty_loss = uncertainty_pred.mean() * 0.01\n",
    "                    \n",
    "                    total_loss = prediction_loss + uncertainty_loss\n",
    "                    total_loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.uncertainty_model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    main_optimizer.step()\n",
    "                    uncertainty_optimizer.step()\n",
    "                \n",
    "                main_scheduler.step()\n",
    "                train_loss += total_loss.item()\n",
    "            \n",
    "            # Update uncertainty scheduler\n",
    "            if epoch % 4 == 0:\n",
    "                uncertainty_scheduler.step()\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            self.uncertainty_model.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_features, batch_targets in val_loader:\n",
    "                    batch_features = batch_features.to(self.device, non_blocking=True)\n",
    "                    batch_targets = batch_targets.to(self.device, non_blocking=True)\n",
    "                    \n",
    "                    if scaler is not None:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            delta_pred = self.model(batch_features)\n",
    "                            loss = mse_loss(delta_pred, batch_targets)\n",
    "                    else:\n",
    "                        delta_pred = self.model(batch_features)\n",
    "                        loss = mse_loss(delta_pred, batch_targets)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            # Store history\n",
    "            self.training_history['loss'].append(train_loss)\n",
    "            self.training_history['val_loss'].append(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin-ai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
